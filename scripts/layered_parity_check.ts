
import * as fs from 'fs';
import * as path from 'path';
import { execSync } from 'child_process';
import type { BNGLModel, GeneratorProgress } from '../types.ts';
import { parseBNGL } from '../services/parseBNGL.ts';
import { generateExpandedNetwork } from '../services/simulation/NetworkExpansion.ts';
import { getSimulationOptionsFromParsedModel } from '../src/utils/simulationOptions.ts';
import { simulate } from '../services/simulation/SimulationLoop.ts';
import { BNGLParser } from '../src/services/graph/core/BNGLParser.ts';
import { GraphCanonicalizer } from '../src/services/graph/core/Canonical.ts';

interface ParsedParameter {
  index: number;
  name: string;
  expression: string;
  value: number;
}

interface ParsedSpecies {
  index: number;
  name: string;
  concentration: number;
}

interface ParsedReaction {
  index: number;
  reactants: number[];
  products: number[];
  rateLawString: string;
  rateValue: number;
}

interface GroupEntry {
  speciesIndex: number;
  coefficient: number;
}

interface ParsedGroup {
  index: number;
  name: string;
  entries: GroupEntry[];
}

interface ParsedNet {
  parameters: ParsedParameter[];
  species: ParsedSpecies[];
  reactions: ParsedReaction[];
  groups: ParsedGroup[];
  paramMap: Map<string, number>;
}

interface DatPoint {
  time: number;
  values: Map<string, number>;
}

interface ParamDiff { name: string; bng2: number; web: number; relErr: number }
interface SpeciesDiff {
  kind: 'missing_in_web' | 'missing_in_bng2' | 'concentration_mismatch';
  name: string;
  bng2Conc?: number;
  webConc?: number;
}
interface ReactionDiff {
  kind: 'missing_in_web' | 'missing_in_bng2' | 'rate_mismatch' | 'multiplicity_mismatch';
  signature: string;
  bng2Rate?: string;
  webRate?: string;
  bng2Value?: number;
  webValue?: number;
  relErr?: number;
}
interface GroupDiff {
  kind: 'missing_in_web' | 'missing_in_bng2' | 'entries_mismatch';
  name: string;
  details?: string;
}
interface TrajectoryDiff {
  observable: string;
  maxRelErr: number;
  maxAbsErr: number;
  firstBadTime: number;
  tier: 'pass' | 'fp_drift' | 'derivative_bug' | 'major';
}

type RootCause =
  | 'pass'
  | 'threshold_only'
  | 'trajectory_accuracy_mismatch'
  | 'parameter_mismatch'
  | 'species_mismatch'
  | 'reaction_count_mismatch'
  | 'rate_constant_mismatch'
  | 'group_mismatch'
  | 'solver_or_steadystate'
  | 'unknown';

interface LayeredReport {
  model: string;
  simulationMethod: 'ode' | 'ssa' | 'nfsim' | 'unspecified' | 'missing';
  rootCause: RootCause;
  parameterDiffs: ParamDiff[];
  speciesDiffs: SpeciesDiff[];
  reactionDiffs: ReactionDiff[];
  groupDiffs: GroupDiff[];
  cdatDiffs: TrajectoryDiff[];
  gdatDiffs: TrajectoryDiff[];
  netFilesCompared: boolean;
  cdatFilesCompared: boolean;
  gdatFilesCompared: boolean;
  firstDivergingLayer: 'none' | 'parameters' | 'species' | 'reactions' | 'groups' | 'cdat' | 'gdat';
  summary: string;
}

interface ModelFiles {
  bnglPath: string | null;
  bng2Net: string | null;
  webNet: string | null;
  bng2Cdat: string | null;
  webCdat: string | null;
  bng2Gdat: string | null;
  webGdat: string | null;
}

interface CliOptions {
  models: string[];
  all: boolean;
  outPath: string;
  generateWebNet: boolean;
  generateWebCdat: boolean;
  verbose: boolean;
  explicit?: Partial<ModelFiles>;
  limit?: number;
  timeoutMs?: number;
}

function detectSimMethod(bnglPath: string | null): 'ode' | 'ssa' | 'nfsim' | 'unspecified' | 'missing' {
  if (!bnglPath || !fs.existsSync(bnglPath)) return 'missing';
  const text = fs.readFileSync(bnglPath, 'utf8').toLowerCase();
  const normalized = text.replace(/\s+/g, '');

  const hasSSA =
    /simulate_ssa\s*\(/.test(text) ||
    normalized.includes('method=>"ssa"') ||
    normalized.includes("method=>'ssa'");

  const hasNF =
    /simulate_nf\s*\(|nfsim\s*\(/.test(text) ||
    normalized.includes('method=>"nf"') ||
    normalized.includes("method=>'nf'") ||
    normalized.includes('method=>"nfsim"') ||
    normalized.includes("method=>'nfsim'");

  if (hasSSA) return 'ssa';
  if (hasNF) return 'nfsim';
  if (/simulate_ode\s*\(/.test(text) || normalized.includes('method=>"ode"') || normalized.includes("method=>'ode'")) return 'ode';
  return 'unspecified';
}

function hasSimulateCommand(bnglPath: string | null): boolean {
  if (!bnglPath || !fs.existsSync(bnglPath)) return false;
  const text = fs.readFileSync(bnglPath, 'utf8');
  return /\bsimulate(?:_ode|_ssa|_nf)?\s*\(/i.test(text);
}

const ROOT = process.cwd();
const ABS_EXACT = 1e-12;
const ABS_TOL_SOLVER = 1e-6;
const ABS_TOL_DERIVATIVE = 5e-6;
const REL_TOL_EXACT = 1e-10;
const REL_TOL_SOLVER = 1e-4;
const REL_TOL_BUG = 1e-2;

const PROJECT_ROOT = ROOT;
const WEB_OUTPUT_DIR = path.join(PROJECT_ROOT, 'web_output');
const BNG_OUTPUT_DIR = path.join(PROJECT_ROOT, 'tests', 'fixtures', 'gdat');
const PUBLIC_MODELS_DIR = path.join(PROJECT_ROOT, 'public', 'models');
const EXAMPLE_MODELS_DIR = path.join(PROJECT_ROOT, 'example-models');

let VERBOSE = true; // Can be overridden with CLI flags

function log(msg: string, level: 'info' | 'warn' | 'error' | 'debug' = 'info') {
  if (!VERBOSE && (level === 'info' || level === 'debug')) return;
  const prefix = level === 'info' ? '' : `[${level.toUpperCase()}] `;
  console.log(`${prefix}${msg}`); 
}

function dedupeModels(models: string[]): string[] {
  const byKey = new Map<string, string>();
  for (const model of models) {
    const key = normalizeKey(model);
    if (!key) continue;
    if (!byKey.has(key)) {
      byKey.set(key, model);
    }
  }
  return [...byKey.values()].sort((a, b) => a.localeCompare(b, undefined, { sensitivity: 'base' }));
}

function normalizeKey(raw: string): string {
  const base = path.basename(raw);
  return base
    .toLowerCase()
    .replace(/^results_/, '')
    .replace(/\.(csv|gdat|bngl|net|cdat)$/i, '')
    .replace(/\(\d+\)$/, '')
    .replace(/\s+/g, '')
    .replace(/[^a-z0-9]+/g, '');
}

function csvModelLabel(csvFile: string): string {
  return csvFile
    .replace(/\(\d+\)(?=\.[^.]+$)/, '')
    .replace(/^results_/, '')
    .replace(/\.csv$/i, '');
}

function relErr(a: number, b: number): number {
  const denom = Math.max(Math.abs(a), Math.abs(b), 1e-30);
  return Math.abs(a - b) / denom;
}

function escapeRegExp(s: string): string {
  return s.replace(/[.*+?^${}()|[\]\\]/g, '\\$&');
}

function evalParamExpr(expr: string, paramMap: Map<string, number>): number {
  const direct = Number(expr);
  if (!Number.isNaN(direct)) return direct;

  if (paramMap.has(expr)) return paramMap.get(expr)!;

  let resolved = expr;
  const sorted = [...paramMap.entries()].sort((a, b) => b[0].length - a[0].length);
  for (const [name, val] of sorted) {
    const re = new RegExp(`\\b${escapeRegExp(name)}\\b`, 'g');
    resolved = resolved.replace(re, `(${val})`);
  }
  resolved = resolved.replace(/\bln\s*\(/g, 'Math.log(');
  resolved = resolved.replace(/\^/g, '**');

  try {
    const fn = new Function('Math', `"use strict"; return (${resolved});`);
    const v = fn(Math);
    if (typeof v === 'number' && Number.isFinite(v)) return v;
  } catch {
    // ignore
  }
  return NaN;
}

function extractBlock(lines: string[], blockName: string): string[] {
  const out: string[] = [];
  let inside = false;
  const beginRe = new RegExp(`^\\s*begin\\s+${blockName}\\s*$`, 'i');
  const endRe = new RegExp(`^\\s*end\\s+${blockName}\\s*$`, 'i');

  for (const raw of lines) {
    const line = raw.replace(/#.*$/, '').trim();
    if (!inside) {
      if (beginRe.test(line)) inside = true;
      continue;
    }
    if (endRe.test(line)) break;
    if (line.length > 0) out.push(line);
  }
  return out;
}

function parseNetFile(content: string): ParsedNet {
  const lines = content.split(/\r?\n/);
  const paramMap = new Map<string, number>();
  const parameters: ParsedParameter[] = [];
  const paramRaw = new Map<string, string>();

  for (const line of extractBlock(lines, 'parameters')) {
    const tokens = line.split(/\s+/);
    if (tokens.length < 2) continue;
    let index = 0;
    let name = '';
    let expr = '';
    if (/^\d+$/.test(tokens[0])) {
      index = parseInt(tokens[0], 10);
      name = tokens[1];
      expr = tokens.slice(2).join(' ');
    } else {
      name = tokens[0];
      expr = tokens.slice(1).join(' ');
    }
    paramRaw.set(name, expr);
    parameters.push({ index, name, expression: expr, value: NaN });
  }

  for (let pass = 0; pass < 10; pass++) {
    let progress = false;
    for (const [name, expr] of paramRaw.entries()) {
      if (paramMap.has(name)) continue;
      const value = evalParamExpr(expr, paramMap);
      if (!Number.isNaN(value)) {
        paramMap.set(name, value);
        const p = parameters.find((x) => x.name === name);
        if (p) p.value = value;
        progress = true;
      }
    }
    if (!progress) break;
  }

  const species: ParsedSpecies[] = [];
  for (const line of extractBlock(lines, 'species')) {
    const tokens = line.split(/\s+/);
    if (tokens.length < 3) continue;
    const idx = parseInt(tokens[0], 10);
    const name = tokens[1].replace(/^\$/, '');
    const concExpr = tokens.slice(2).join(' ');
    const concentration = evalParamExpr(concExpr, paramMap);
    species.push({ index: idx, name, concentration });
  }

  const reactions: ParsedReaction[] = [];
  for (const line of extractBlock(lines, 'reactions')) {
    const tokens = line.split(/\s+/);
    if (tokens.length < 3) continue;
    const idx = parseInt(tokens[0], 10);
    if (!Number.isFinite(idx)) continue;
    const reactants = tokens[1].split(',').map(Number).filter((n) => n > 0).sort((a, b) => a - b);

    let products: number[] = [];
    let rateLawString = '';
    if (tokens.length === 3) {
      products = [0];
      rateLawString = tokens[2];
    } else {
      products = tokens[2].split(',').map(Number).filter((n) => n > 0).sort((a, b) => a - b);
      rateLawString = tokens.slice(3).join(' ');
    }

    const rateValue = evalParamExpr(rateLawString, paramMap);
    reactions.push({ index: idx, reactants, products, rateLawString, rateValue });
  }

  const groups: ParsedGroup[] = [];
  for (const line of extractBlock(lines, 'groups')) {
    const tokens = line.split(/\s+/);
    if (tokens.length < 3) continue;
    const idx = parseInt(tokens[0], 10);
    const name = tokens[1];
    const entriesStr = tokens.slice(2).join('');
    const entries: GroupEntry[] = [];

    for (const part of entriesStr.split(',')) {
      const trimmed = part.trim();
      if (!trimmed) continue;
      const star = trimmed.indexOf('*');
      if (star >= 0) {
        const coeff = Number.parseFloat(trimmed.slice(0, star));
        const specIdx = Number.parseInt(trimmed.slice(star + 1), 10);
        if (!Number.isNaN(coeff) && !Number.isNaN(specIdx) && specIdx > 0) {
          entries.push({ speciesIndex: specIdx, coefficient: coeff });
        }
      } else {
        const specIdx = Number.parseInt(trimmed, 10);
        if (!Number.isNaN(specIdx) && specIdx > 0) {
          entries.push({ speciesIndex: specIdx, coefficient: 1 });
        }
      }
    }

    groups.push({ index: idx, name, entries });
  }

  return { parameters, species, reactions, groups, paramMap };
}

function parseDat(content: string): DatPoint[] {
  const lines = content.split(/\r?\n/).filter((l) => l.trim().length > 0);
  if (lines.length < 2) return [];

  let headerLine = lines[0];
  if (headerLine.startsWith('#')) headerLine = headerLine.slice(1);
  const headers = headerLine.trim().split(/\s+/);

  const points: DatPoint[] = [];
  for (let i = 1; i < lines.length; i++) {
    if (lines[i].startsWith('#')) continue;
    const valuesArray = lines[i].trim().split(/\s+/).map(Number);
    if (valuesArray.length < 2) continue;
    const values = new Map<string, number>();
    for (let j = 0; j < headers.length && j < valuesArray.length; j++) {
      values.set(headers[j], valuesArray[j]);
    }
    points.push({ time: valuesArray[0], values });
  }
  return points;
}

function parseCsv(content: string): DatPoint[] {
  const lines = content.split(/\r?\n/).filter((l) => l.trim().length > 0);
  if (lines.length < 2) return [];
  const headers = lines[0].split(',').map((h) => h.trim());
  const points: DatPoint[] = [];

  for (let i = 1; i < lines.length; i++) {
    const parts = lines[i].split(',').map((x) => Number(x.trim()));
    if (parts.length < 2) continue;
    const values = new Map<string, number>();
    for (let j = 0; j < headers.length && j < parts.length; j++) values.set(headers[j], parts[j]);
    points.push({ time: parts[0], values });
  }

  return points;
}

function reactionSignature(rxn: ParsedReaction, speciesList: ParsedSpecies[]): string {
  const idxToName = new Map(speciesList.map((s) => [s.index, canonicalizePattern(s.name)]));
  const lhs = rxn.reactants.map((i) => idxToName.get(i) ?? `?${i}`).sort().join(' + ');
  const rhs = rxn.products.map((i) => idxToName.get(i) ?? `?${i}`).sort().join(' + ');
  return `${lhs} -> ${rhs}`;
}

function isSyntheticTimerSpecies(name: string): boolean {
  const normalized = canonicalizePattern(name);
  const lowered = normalized.toLowerCase();
  return (
    lowered === 'timer()' ||
    /(^|\.)timer\(\)$/.test(lowered) ||
    lowered.includes('__timer') ||
    lowered.includes('timer_species')
  );
}

function isSyntheticTimerReaction(signature: string): boolean {
  const compact = signature.replace(/\s+/g, '').toLowerCase();
  return /(^|[+\-]|->)timer\(\)(?=$|[+\-]|->)/.test(compact) || compact.includes('__timer');
}

const CANONICAL_CACHE = new Map<string, string>();

function canonicalizePattern(pattern: string): string {
  const trimmed = pattern.trim();
  if (!trimmed || trimmed === '0') return trimmed;
  if (CANONICAL_CACHE.has(trimmed)) return CANONICAL_CACHE.get(trimmed)!;

  const normalizedForParse = trimmed.replace(/\bs_type(?=~)/g, 'type');

  let result: string;
  try {
    const graph = BNGLParser.parseSpeciesGraph(normalizedForParse);
    result = GraphCanonicalizer.canonicalize(graph);
  } catch {
    result = normalizedForParse.replace(/\s+/g, '').replace(/::/g, ':');
  }
  CANONICAL_CACHE.set(trimmed, result);
  return result;
}

function warmCanonicalCache(species: ParsedSpecies[], label: string) {
  if (species.length > 20) {
    console.log(`    [Progress] Canonicalizing ${species.length} species for ${label}...`);
  }
  let count = 0;
  for (const s of species) {
    count++;
    if (count % 10 === 0) console.log(`      ... processed ${count}/${species.length}`);
    canonicalizePattern(s.name);
  }
}

function compareParameters(bng2: ParsedNet, web: ParsedNet): ParamDiff[] {
  const out: ParamDiff[] = [];
  const webMap = new Map(web.parameters.map((p) => [p.name, p.value]));

  for (const p of bng2.parameters) {
    const w = webMap.get(p.name);
    if (w === undefined || Number.isNaN(w) || Number.isNaN(p.value)) continue;
    const e = relErr(p.value, w);
    if (e > REL_TOL_EXACT) out.push({ name: p.name, bng2: p.value, web: w, relErr: e });
  }

  return out.sort((a, b) => b.relErr - a.relErr);
}

function compareSpecies(bng2: ParsedNet, web: ParsedNet): SpeciesDiff[] {
  const out: SpeciesDiff[] = [];
  const norm = (s: string) => canonicalizePattern(s);
  const bMap = new Map(bng2.species.map((s) => [norm(s.name), s]));
  const wMap = new Map(web.species.map((s) => [norm(s.name), s]));

  if (VERBOSE) {
    log(`      [Debug] BNG2 Seed Species:`);
    bng2.species.filter(s => s.concentration > 0).forEach(s => log(`        ${s.name} (norm=${norm(s.name)}) conc=${s.concentration}`));
    log(`      [Debug] Web Seed Species:`);
    web.species.filter(s => s.concentration > 0).forEach(s => log(`        ${s.name} (norm=${norm(s.name)}) conc=${s.concentration}`));
  }

  for (const [name, bs] of bMap.entries()) {
    const ws = wMap.get(name);
    if (!ws) {
      out.push({ kind: 'missing_in_web', name: bs.name, bng2Conc: bs.concentration });
      continue;
    }
    if (!Number.isNaN(bs.concentration) && !Number.isNaN(ws.concentration) && relErr(bs.concentration, ws.concentration) > REL_TOL_EXACT) {
      out.push({ kind: 'concentration_mismatch', name: bs.name, bng2Conc: bs.concentration, webConc: ws.concentration });
    }
  }

  for (const [name, ws] of wMap.entries()) {
    if (!bMap.has(name)) {
      if (isSyntheticTimerSpecies(ws.name)) continue;
      out.push({ kind: 'missing_in_bng2', name: ws.name, webConc: ws.concentration });
    }
  }

  return out;
}

function compareReactions(bng2: ParsedNet, web: ParsedNet): ReactionDiff[] {
  const out: ReactionDiff[] = [];
  const bCountBySignature = new Map<string, number>();
  const wCountBySignature = new Map<string, number>();

  const bAgg = new Map<string, ParsedReaction>();
  for (const r of bng2.reactions) {
    const sig = reactionSignature(r, bng2.species);
    if (isSyntheticTimerReaction(sig)) continue;
    bCountBySignature.set(sig, (bCountBySignature.get(sig) ?? 0) + 1);
    const prev = bAgg.get(sig);
    if (!prev) {
      bAgg.set(sig, { ...r });
    } else if (!Number.isNaN(prev.rateValue) && !Number.isNaN(r.rateValue)) {
      prev.rateValue += r.rateValue;
      prev.rateLawString += ` + ${r.rateLawString}`;
    }
  }

  const wAgg = new Map<string, ParsedReaction>();
  for (const r of web.reactions) {
    const sig = reactionSignature(r, web.species);
    if (isSyntheticTimerReaction(sig)) continue;
    wCountBySignature.set(sig, (wCountBySignature.get(sig) ?? 0) + 1);
    const prev = wAgg.get(sig);
    if (!prev) {
      wAgg.set(sig, { ...r });
    } else if (!Number.isNaN(prev.rateValue) && !Number.isNaN(r.rateValue)) {
      prev.rateValue += r.rateValue;
      prev.rateLawString += ` + ${r.rateLawString}`;
    }
  }

  for (const [sig, br] of bAgg.entries()) {
    const wr = wAgg.get(sig);
    if (!wr) {
      out.push({ kind: 'missing_in_web', signature: sig, bng2Rate: br.rateLawString, bng2Value: br.rateValue });
      continue;
    }
    if (!Number.isNaN(br.rateValue) && !Number.isNaN(wr.rateValue)) {
      const e = relErr(br.rateValue, wr.rateValue);
      if (e > REL_TOL_EXACT) {
        out.push({
          kind: 'rate_mismatch',
          signature: sig,
          bng2Rate: br.rateLawString,
          webRate: wr.rateLawString,
          bng2Value: br.rateValue,
          webValue: wr.rateValue,
          relErr: e,
        });
      }
    }

    const bCount = bCountBySignature.get(sig) ?? 0;
    const wCount = wCountBySignature.get(sig) ?? 0;
    if (bCount !== wCount) {
      out.push({
        kind: 'multiplicity_mismatch',
        signature: sig,
        bng2Rate: `count=${bCount}`,
        webRate: `count=${wCount}`,
      });
    }
  }

  for (const [sig, wr] of wAgg.entries()) {
    if (!bAgg.has(sig)) {
      out.push({ kind: 'missing_in_bng2', signature: sig, webRate: wr.rateLawString, webValue: wr.rateValue });
    }
  }

  return out.sort((a, b) => (b.relErr ?? 0) - (a.relErr ?? 0));
}

function compareGroups(bng2: ParsedNet, web: ParsedNet): GroupDiff[] {
  const out: GroupDiff[] = [];
  const wMap = new Map(web.groups.map((g) => [g.name, g]));

  for (const bg of bng2.groups) {
    const wg = wMap.get(bg.name);
    if (!wg) {
      out.push({ kind: 'missing_in_web', name: bg.name });
      continue;
    }

    const bNames = new Map(bng2.species.map((s) => [s.index, canonicalizePattern(s.name)]));
    const wNames = new Map(web.species.map((s) => [s.index, canonicalizePattern(s.name)]));

    const bByName = new Map<string, number>();
    for (const e of bg.entries) {
      const n = bNames.get(e.speciesIndex) ?? `?${e.speciesIndex}`;
      bByName.set(n, (bByName.get(n) ?? 0) + e.coefficient);
    }

    const wByName = new Map<string, number>();
    for (const e of wg.entries) {
      const n = wNames.get(e.speciesIndex) ?? `?${e.speciesIndex}`;
      wByName.set(n, (wByName.get(n) ?? 0) + e.coefficient);
    }

    const names = new Set([...bByName.keys(), ...wByName.keys()]);
    const diffs: string[] = [];
    for (const n of names) {
      const bv = bByName.get(n) ?? 0;
      const wv = wByName.get(n) ?? 0;
      if (Math.abs(bv - wv) > ABS_EXACT) diffs.push(`${n}: bng2=${bv}, web=${wv}`);
    }

    if (diffs.length > 0) {
      out.push({ kind: 'entries_mismatch', name: bg.name, details: diffs.join('; ') });
    }
  }

  for (const wg of web.groups) {
    if (!bng2.groups.find((g) => g.name === wg.name)) out.push({ kind: 'missing_in_bng2', name: wg.name });
  }

  return out;
}

function compareTimeSeries(bng2Points: DatPoint[], webPoints: DatPoint[]): TrajectoryDiff[] {
  if (bng2Points.length === 0 || webPoints.length === 0) return [];

  const bNames = new Set([...bng2Points[0].values.keys()].filter((k) => k.toLowerCase() !== 'time'));
  const wNames = new Set([...webPoints[0].values.keys()].filter((k) => k.toLowerCase() !== 'time'));
  const common = [...bNames].filter((n) => wNames.has(n));

  const findClosestWeb = (t: number): DatPoint | null => {
    let best: DatPoint | null = null;
    let bestDist = Infinity;
    let bestIndex = -1;
    for (let i = 0; i < webPoints.length; i++) {
      const wp = webPoints[i];
      const d = Math.abs(wp.time - t);
      if (d < bestDist || (Math.abs(d - bestDist) <= 1e-12 && i > bestIndex)) {
        bestDist = d;
        best = wp;
        bestIndex = i;
      }
    }
    const tSpan = Math.max(1e-10, Math.abs(bng2Points[bng2Points.length - 1].time - bng2Points[0].time));
    return bestDist <= 0.01 * tSpan ? best : null;
  };

  const out: TrajectoryDiff[] = [];

  for (const obs of common) {
    let maxRelErr = 0;
    let maxAbsErr = 0;
    let firstBad = Infinity;

    for (const bp of bng2Points) {
      const wp = findClosestWeb(bp.time);
      if (!wp) continue;
      const bv = bp.values.get(obs);
      const wv = wp.values.get(obs);
      if (bv === undefined || wv === undefined) continue;

      const abs = Math.abs(bv - wv);
      const rel = relErr(bv, wv);
      if (abs > maxAbsErr) maxAbsErr = abs;
      if (rel > maxRelErr) maxRelErr = rel;
      if (rel > REL_TOL_SOLVER && abs > ABS_TOL_DERIVATIVE && bp.time < firstBad) firstBad = bp.time;
    }

    const absDominated = maxAbsErr <= ABS_TOL_SOLVER;
    let tier: TrajectoryDiff['tier'];
    if (absDominated || maxRelErr <= REL_TOL_EXACT) tier = 'pass';
    else if (maxRelErr <= REL_TOL_SOLVER || maxAbsErr <= ABS_TOL_DERIVATIVE) tier = 'fp_drift';
    else if (maxRelErr <= REL_TOL_BUG) tier = 'derivative_bug';
    else tier = 'major';

    out.push({
      observable: obs,
      maxRelErr,
      maxAbsErr,
      firstBadTime: firstBad === Infinity ? -1 : firstBad,
      tier,
    });
  }

  return out.sort((a, b) => b.maxRelErr - a.maxRelErr);
}

function classifyRootCause(
  partial: Omit<LayeredReport, 'rootCause' | 'summary' | 'firstDivergingLayer'>,
  opts?: { simMethod?: 'ode' | 'ssa' | 'nfsim' | 'unspecified' | 'missing' }
): { rootCause: RootCause; firstDivergingLayer: LayeredReport['firstDivergingLayer'] } {
  const deterministicLike = (opts?.simMethod ?? 'unspecified') === 'ode' || (opts?.simMethod ?? 'unspecified') === 'unspecified';

  if (partial.parameterDiffs.some((d) => d.relErr > REL_TOL_SOLVER)) return { rootCause: 'parameter_mismatch', firstDivergingLayer: 'parameters' };

  const structuralSpeciesMismatch = partial.speciesDiffs.some((d) => d.kind !== 'concentration_mismatch');
  if (structuralSpeciesMismatch) return { rootCause: 'species_mismatch', firstDivergingLayer: 'species' };

  const missingRxn = partial.reactionDiffs.some((d) =>
    d.kind === 'missing_in_web' || d.kind === 'missing_in_bng2' || d.kind === 'multiplicity_mismatch'
  );
  if (missingRxn) return { rootCause: 'reaction_count_mismatch', firstDivergingLayer: 'reactions' };

  const rateMismatch = partial.reactionDiffs.some((d) => d.kind === 'rate_mismatch' && (d.relErr ?? 0) > REL_TOL_SOLVER);
  if (rateMismatch) return { rootCause: 'rate_constant_mismatch', firstDivergingLayer: 'reactions' };

  const groupMismatch = partial.groupDiffs.some((d) => d.kind === 'entries_mismatch');
  if (groupMismatch) return { rootCause: 'group_mismatch', firstDivergingLayer: 'groups' };

  const majorCdat = partial.cdatDiffs.some((d) => d.tier === 'major');
  if (majorCdat) {
    if (!deterministicLike) return { rootCause: 'unknown', firstDivergingLayer: 'cdat' };
    return { rootCause: partial.netFilesCompared ? 'solver_or_steadystate' : 'unknown', firstDivergingLayer: 'cdat' };
  }

  const majorGdat = partial.gdatDiffs.some((d) => d.tier === 'major');
  if (majorGdat) {
    if (!deterministicLike) return { rootCause: 'unknown', firstDivergingLayer: 'gdat' };
    return { rootCause: partial.netFilesCompared ? 'solver_or_steadystate' : 'unknown', firstDivergingLayer: 'gdat' };
  }

  const derivativeCdat = partial.cdatDiffs.some((d) => d.tier === 'derivative_bug');
  const derivativeGdat = partial.gdatDiffs.some((d) => d.tier === 'derivative_bug');
  if (derivativeCdat || derivativeGdat) {
    const layer: LayeredReport['firstDivergingLayer'] = derivativeCdat ? 'cdat' : 'gdat';
    if (!deterministicLike) return { rootCause: 'unknown', firstDivergingLayer: layer };
    return { rootCause: 'trajectory_accuracy_mismatch', firstDivergingLayer: layer };
  }

  const fpDrift =
    partial.cdatDiffs.some((d) => d.tier === 'fp_drift') ||
    partial.gdatDiffs.some((d) => d.tier === 'fp_drift');
  if (fpDrift) return { rootCause: 'threshold_only', firstDivergingLayer: 'none' };

  return { rootCause: 'pass', firstDivergingLayer: 'none' };
}

function buildSummary(report: LayeredReport): string {
  const worstGdat = report.gdatDiffs[0];
  const worstCdat = report.cdatDiffs[0];
  const worstAny = [worstGdat, worstCdat].filter(Boolean).sort((a, b) => (b!.maxRelErr - a!.maxRelErr))[0];

  return [
    `Model: ${report.model}`,
    `Simulation method: ${report.simulationMethod}`,
    `Root cause: ${report.rootCause}`,
    `First diverging layer: ${report.firstDivergingLayer}`,
    `NET compared: ${report.netFilesCompared} | CDAT compared: ${report.cdatFilesCompared} | GDAT compared: ${report.gdatFilesCompared}`,
    `Diffs: param=${report.parameterDiffs.length}, species=${report.speciesDiffs.length}, rxn=${report.reactionDiffs.length}, groups=${report.groupDiffs.length}`,
    `Worst trajectory error: ${worstAny ? worstAny.maxRelErr.toExponential(3) : 'n/a'}`,
  ].join('\n');
}

function readDirFilesIfExists(dirPath: string, ext: string): string[] {
  if (!fs.existsSync(dirPath)) return [];
  return fs.readdirSync(dirPath)
    .filter((f) => f.toLowerCase().endsWith(ext.toLowerCase()))
    .map((f) => path.join(dirPath, f));
}

function sleepMs(ms: number): void {
  const sab = new SharedArrayBuffer(4);
  const int32 = new Int32Array(sab);
  Atomics.wait(int32, 0, 0, ms);
}

function readTextFileWithRetry(filePath: string, retries = 5, delayMs = 200): string {
  let lastError: any;
  for (let attempt = 0; attempt <= retries; attempt++) {
    try {
      return fs.readFileSync(filePath, 'utf8');
    } catch (error: any) {
      lastError = error;
      const code = error?.code;
      const transientLock = code === 'EBUSY' || code === 'EPERM';
      if (!transientLock || attempt === retries) {
        throw error;
      }
      sleepMs(delayMs * (attempt + 1));
    }
  }
  throw lastError;
}

function findFilesRecursive(dir: string, ext: string): string[] {
  if (!fs.existsSync(dir)) return [];
  let results: string[] = [];
  const list = fs.readdirSync(dir);
  for (const file of list) {
    const filePath = path.join(dir, file);
    const stat = fs.statSync(filePath);
    if (stat && stat.isDirectory()) {
      results = results.concat(findFilesRecursive(filePath, ext));
    } else if (file.toLowerCase().endsWith(ext.toLowerCase())) {
      results.push(filePath);
    }
  }
  return results;
}

function netFileHasContent(filePath: string): boolean {
  if (!fs.existsSync(filePath)) return false;
  const text = fs.readFileSync(filePath, 'utf8');
  const lines = text.split(/\r?\n/);
  const speciesLines = extractBlock(lines, 'species').filter((l) => /^\d+\s+/.test(l));
  const reactionLines = extractBlock(lines, 'reactions').filter((l) => /^\d+\s+/.test(l));
  return speciesLines.length > 0 || reactionLines.length > 0;
}

function bestMatchFile(modelKey: string, candidates: string[]): string | null {
  const exactMatches = candidates.filter((fp) => normalizeKey(path.basename(fp)) === modelKey);
  if (exactMatches.length > 0) {
    exactMatches.sort((a, b) => path.basename(a).length - path.basename(b).length);
    return exactMatches[0];
  }

  let best: { file: string; score: number } | null = null;
  for (const fp of candidates) {
    const base = path.basename(fp);
    const key = normalizeKey(base);
    let score = 0;
    if (key.startsWith(modelKey) || modelKey.startsWith(key)) score += 500;
    else if (key.includes(modelKey) || modelKey.includes(key)) score += 200;
    
    score -= Math.abs(key.length - modelKey.length);
    if (!best || score > best.score) best = { file: fp, score };
  }
  if (!best || best.score < 500) return null; // Require at least a prefix/starts-with match
  return best.file;
}

function discoverBngl(modelName: string): string | null {
  const key = normalizeKey(modelName);
  const candidates = [
    ...readDirFilesIfExists(PUBLIC_MODELS_DIR, '.bngl'),
    ...readDirFilesIfExists(EXAMPLE_MODELS_DIR, '.bngl'),
    ...readDirFilesIfExists(path.join(ROOT, 'tests', 'fixtures', 'bngl'), '.bngl'),
    ...readDirFilesIfExists(path.join(ROOT, 'bng_test_output'), '.bngl'),
  ];
  return bestMatchFile(key, candidates);
}

function discoverFiles(modelName: string): ModelFiles {
  const key = normalizeKey(modelName);
  log(`  [Discovery] Searching for files for model: ${modelName} (key: ${key})`);

  const bng2NetCandidates = [
    ...readDirFilesIfExists(path.join(ROOT, 'bng_test_output'), '.net'),
    ...readDirFilesIfExists(path.join(ROOT, 'tests', 'fixtures', 'net'), '.net'),
    ...readDirFilesIfExists(ROOT, '.net'),
  ];
  const bng2CdatCandidates = [
    ...readDirFilesIfExists(path.join(ROOT, 'bng_test_output'), '.cdat'),
    ...readDirFilesIfExists(path.join(ROOT, 'tests', 'fixtures', 'cdat'), '.cdat'),
    ...readDirFilesIfExists(ROOT, '.cdat'),
  ];
  const bng2GdatCandidates = [
    ...readDirFilesIfExists(path.join(ROOT, 'tests', 'fixtures', 'gdat'), '.gdat'),
    ...readDirFilesIfExists(path.join(ROOT, 'bng_test_output'), '.gdat'),
    ...readDirFilesIfExists(ROOT, '.gdat'),
  ];

  const webNetCandidates = readDirFilesIfExists(WEB_OUTPUT_DIR, '.net').filter((fp) => netFileHasContent(fp));
  const webCdatCandidates = [
    ...readDirFilesIfExists(WEB_OUTPUT_DIR, '.cdat'),
  ];

  const webGdatCandidates = readDirFilesIfExists(WEB_OUTPUT_DIR, '.csv');
  const webGdatCandidate = bestMatchFile(normalizeKey(`results_${modelName}`), webGdatCandidates) ?? bestMatchFile(key, webGdatCandidates);

  const files: ModelFiles = {
    bnglPath: discoverBngl(modelName),
    bng2Net: bestMatchFile(key, bng2NetCandidates),
    webNet: bestMatchFile(key, webNetCandidates),
    bng2Cdat: bestMatchFile(key, bng2CdatCandidates),
    webCdat: bestMatchFile(key, webCdatCandidates),
    bng2Gdat: bestMatchFile(key, bng2GdatCandidates),
    webGdat: webGdatCandidate,
  };

  if (files.bnglPath) log(`    Found BNGL: ${files.bnglPath}`);
  if (files.bng2Net) log(`    Found BNG2 Net: ${files.bng2Net}`);
  if (files.webNet) log(`    Found Web Net: ${files.webNet}`);
  if (files.bng2Cdat) log(`    Found BNG2 Cdat: ${files.bng2Cdat}`);
  if (files.webCdat) log(`    Found Web Cdat: ${files.webCdat}`);
  if (files.bng2Gdat) log(`    Found BNG2 Gdat: ${files.bng2Gdat}`);
  if (files.webGdat) log(`    Found Web Gdat: ${files.webGdat}`);

  return files;
}

async function ensureGeneratedArtifacts(modelName: string, files: ModelFiles, opts: CliOptions): Promise<ModelFiles> {
  if (!files.bnglPath) return files;

  const simulationMethod = detectSimMethod(files.bnglPath);
  const deterministicLike = simulationMethod === 'ode' || simulationMethod === 'unspecified';
  const needNet = opts.generateWebNet;
  const needCdat = opts.generateWebCdat && !files.webCdat && deterministicLike;

  if (opts.generateWebCdat && !deterministicLike) {
    log(`    [Artifacts] Skipping internal CDAT/GDAT generation for ${modelName} (method=${simulationMethod})`);
  }

  if (!needNet && !needCdat) return files;

  const bnglText = fs.readFileSync(files.bnglPath, 'utf8');
  const parsed = parseBNGL(bnglText);

  fs.mkdirSync(WEB_OUTPUT_DIR, { recursive: true });

  const safeModelName = modelName.replace(/[^a-zA-Z0-9]/g, '_');

  if (needNet) {
    const netPath = path.join(WEB_OUTPUT_DIR, `${safeModelName}.net`);
    log(`    [Artifacts] Exporting network for ${modelName} to ${netPath}...`);
    try {
      const cmd = `npx -y tsx scripts/export_net.ts "${files.bnglPath}" "${netPath}"`;
      log(`    Executing: ${cmd}`);
      execSync(cmd, { cwd: ROOT, stdio: VERBOSE ? 'inherit' : 'pipe', maxBuffer: 50 * 1024 * 1024 });
      if (netFileHasContent(netPath)) {
        files.webNet = netPath;
        log(`    Successfully generated network file.`);
      } else {
        log(`    Network file generated but is empty or invalid.`, 'error');
        files.webNet = null;
        try { fs.unlinkSync(netPath); } catch {}
      }
    } catch (e: any) {
      log(`    Failed to generate network via export_net.ts: ${e.message}`, 'error');
      files.webNet = null;
    }
  }

  if (needCdat) {
    log(`    [Artifacts] Running internal simulation for ${modelName}...`);
    try {
      const expanded = await generateExpandedNetwork(parsed, () => {}, () => {});
      log(`    Expansion complete: ${expanded.species.length} species, ${expanded.reactions.length} reactions.`);
      const simOptions = getSimulationOptionsFromParsedModel(expanded, 'ode', {
        solver: 'cvode',
      });
      log(`    Starting simulation with solver=${simOptions.solver}, t_end=${simOptions.t_end}, n_steps=${simOptions.n_steps}`);
      const simResult = await simulate(0, expanded, simOptions, {
        checkCancelled: () => {},
        postMessage: (() => {}) as any,
      });

      const headers = expanded.species.map((_, i) => `S${i + 1}`);
      const rows = (simResult.speciesData ?? []).map((row: any) => {
        const vals = [String(row.time ?? 0)];
        for (const s of expanded.species) {
          vals.push(String(row[s.name] ?? 0));
        }
        return vals.join(' ');
      });

      const lines = [`# time ${headers.join(' ')}`, ...rows];
      const cdatPath = path.join(WEB_OUTPUT_DIR, `${safeModelName}.cdat`);
      const gdatPath = path.join(WEB_OUTPUT_DIR, `results_${safeModelName}.csv`);
      fs.mkdirSync(path.dirname(cdatPath), { recursive: true });
      fs.writeFileSync(cdatPath, lines.join('\n') + '\n', 'utf8');
      files.webCdat = cdatPath;

      // Generate GDAT (observable trajectories)
      const obsHeaders = (simResult.headers ?? []).filter((h: string) => h !== 'time');
      const obsRows = (simResult.data ?? []).map((row: any) => {
        const vals = [String(row.time ?? 0)];
        for (const h of obsHeaders) vals.push(String(row[h] ?? 0));
        return vals.join(',');
      });
      const gdatLines = [`time,${obsHeaders.join(',')}`, ...obsRows];
      fs.writeFileSync(gdatPath, gdatLines.join('\n') + '\n', 'utf8');
      files.webGdat = gdatPath;

      log(`    Successfully generated simulation artifacts (CDAT, GDAT).`);
    } catch (e: any) {
      log(`    Failed to run internal simulation: ${e.message}`, 'error');
      files.webCdat = null;
    }
  }

  return files;
}

function alignCdat(webPoints: DatPoint[], webNet: ParsedNet, bng2Net: ParsedNet): DatPoint[] {
  const webToCanonical = new Map<string, string>();
  for (const s of webNet.species) {
    webToCanonical.set(`S${s.index}`, canonicalizePattern(s.name));
  }
  
  const canonicalToBng2 = new Map<string, string>();
  for (const s of bng2Net.species) {
    canonicalToBng2.set(canonicalizePattern(s.name), `S${s.index}`);
  }

  return webPoints.map(p => {
    const newValues = new Map<string, number>();
    for (const [key, val] of p.values.entries()) {
      if (key.startsWith('S')) {
        const can = webToCanonical.get(key);
        const bng2Key = can ? canonicalToBng2.get(can) : null;
        if (bng2Key) {
          newValues.set(bng2Key, val);
        } else {
          newValues.set(key, val);
        }
      } else {
        newValues.set(key, val);
      }
    }
    return { ...p, values: newValues };
  });
}

function rescaleCdatForCompartments(bng2Points: DatPoint[], webPoints: DatPoint[], bng2Net: ParsedNet): DatPoint[] {
  const hasCompartments = bng2Net.species.some((s) => s.name.trim().startsWith('@'));
  if (!hasCompartments) return webPoints;
  if (bng2Points.length === 0 || webPoints.length === 0) return webPoints;

  const keys = [...bng2Points[0].values.keys()].filter((k) => /^S\d+$/.test(k));
  const scaleByKey = new Map<string, number>();
  const EPS = 1e-15;

  for (const key of keys) {
    let ratio: number | null = null;
    const n = Math.min(bng2Points.length, webPoints.length);
    for (let i = 0; i < n; i++) {
      const b = bng2Points[i].values.get(key) ?? 0;
      const w = webPoints[i].values.get(key) ?? 0;
      if (Math.abs(b) > EPS && Math.abs(w) > EPS) {
        ratio = b / w;
        break;
      }
    }

    if (ratio !== null && Number.isFinite(ratio) && Math.abs(ratio) > EPS) {
      scaleByKey.set(key, ratio);
    }
  }

  if (scaleByKey.size === 0) return webPoints;

  return webPoints.map((p) => {
    const newValues = new Map<string, number>();
    for (const [key, val] of p.values.entries()) {
      if (/^S\d+$/.test(key)) {
        const scale = scaleByKey.get(key) ?? 1;
        newValues.set(key, val * scale);
      } else {
        newValues.set(key, val);
      }
    }
    return { ...p, values: newValues };
  });
}

async function analyzeModel(modelName: string, files: ModelFiles, opts: CliOptions): Promise<LayeredReport> {
  const preparedFiles = await ensureGeneratedArtifacts(modelName, files, opts);
  const simulationMethod = detectSimMethod(preparedFiles.bnglPath);
  const deterministicLike = simulationMethod === 'ode' || simulationMethod === 'unspecified';

  let parameterDiffs: ParamDiff[] = [];
  let speciesDiffs: SpeciesDiff[] = [];
  let reactionDiffs: ReactionDiff[] = [];
  let groupDiffs: GroupDiff[] = [];
  let bng2Net: ParsedNet | undefined;
  let webNet: ParsedNet | undefined;
  let cdatDiffs: TrajectoryDiff[] = [];
  let gdatDiffs: TrajectoryDiff[] = [];

  let netFilesCompared = !!(preparedFiles.bng2Net && preparedFiles.webNet);
  let cdatFilesCompared = !!(preparedFiles.bng2Cdat && preparedFiles.webCdat);
  let gdatFilesCompared = !!(preparedFiles.bng2Gdat && preparedFiles.webGdat);

  if (netFilesCompared) {
    log(`    [Parity] Comparing static network layers (params, species, rxns, groups)...`);
    const bng2Content = readTextFileWithRetry(preparedFiles.bng2Net!);
    const webContent = readTextFileWithRetry(preparedFiles.webNet!);
    log(`      Read BNG2 Net: ${preparedFiles.bng2Net} (${bng2Content.length} bytes)`);
    log(`      Read Web Net:  ${preparedFiles.webNet} (${webContent.length} bytes)`);

    bng2Net = parseNetFile(bng2Content);
    webNet = parseNetFile(webContent);

    log(`      BNG2 Net: ${bng2Net.parameters.length} params, ${bng2Net.species.length} species, ${bng2Net.reactions.length} rxns, ${bng2Net.groups.length} groups`);
    log(`      Web Net:  ${webNet.parameters.length} params, ${webNet.species.length} species, ${webNet.reactions.length} rxns, ${webNet.groups.length} groups`);

    warmCanonicalCache(bng2Net.species, 'BNG2 reference');
    warmCanonicalCache(webNet.species, 'Web export');

    parameterDiffs = compareParameters(bng2Net, webNet);
    speciesDiffs = compareSpecies(bng2Net, webNet);
    reactionDiffs = compareReactions(bng2Net, webNet);
    groupDiffs = compareGroups(bng2Net, webNet);
    // ...

    if (parameterDiffs.length > 0) {
      log(`      [Diffs] Top Parameter Mismatches:`);
      parameterDiffs.slice(0, 5).forEach(d => log(`        ${d.name}: bng2=${d.bng2}, web=${d.web} (relErr=${d.relErr.toExponential(2)})`));
    }
    if (speciesDiffs.length > 0) {
      log(`      [Diffs] Top Species Mismatches:`);
      speciesDiffs.slice(0, 5).forEach(d => log(`        ${d.name}: kind=${d.kind}, bng2=${d.bng2Conc ?? 'n/a'}, web=${d.webConc ?? 'n/a'}`));
    }
    if (reactionDiffs.length > 0) {
      log(`      [Diffs] Top Reaction Mismatches:`);
      reactionDiffs.slice(0, 5).forEach(d => log(`        ${d.signature}: kind=${d.kind}, bng2Rate=${d.bng2Rate}, webRate=${d.webRate}, relErr=${d.relErr?.toExponential(2) ?? 'n/a'}`));
    }
    if (groupDiffs.length > 0) {
      log(`      [Diffs] Top Group Mismatches:`);
      groupDiffs.slice(0, 5).forEach(d => log(`        ${d.name}: kind=${d.kind}, details=${d.details}`));
    }
  } else {
    log(`    [Parity] Skipping static network layers (missing NET files)`);
    if (!preparedFiles.bng2Net) log(`      Missing BNG2 NET`);
    if (!preparedFiles.webNet) log(`      Missing Web NET`);
  }

  if (cdatFilesCompared && deterministicLike) {
    log(`    [Parity] Comparing concentration trajectories (CDAT)...`);
    const bng2CdatRaw = readTextFileWithRetry(preparedFiles.bng2Cdat!);
    const webCdatRaw = readTextFileWithRetry(preparedFiles.webCdat!);
    log(`      Read BNG2 CDAT: ${preparedFiles.bng2Cdat} (${bng2CdatRaw.length} bytes)`);
    log(`      Read Web CDAT:  ${preparedFiles.webCdat} (${webCdatRaw.length} bytes)`);
    
    const bng2Cdat = parseDat(bng2CdatRaw);
    let webCdat = parseDat(webCdatRaw);
    log(`      BNG2 Dat: ${bng2Cdat.length} timepoints, Web Dat: ${webCdat.length} timepoints`);
    
    if (bng2Net && webNet) {
      log(`      Aligning Web CDAT species indices to BNG2 species indices...`);
      webCdat = alignCdat(webCdat, webNet, bng2Net);
      const hasCompartments = bng2Net.species.some((s) => s.name.trim().startsWith('@'));
      if (hasCompartments) {
        const rawWorst = (compareTimeSeries(bng2Cdat, webCdat)[0]?.maxRelErr ?? 0);
        const rescaled = rescaleCdatForCompartments(bng2Cdat, webCdat, bng2Net);
        const rescaledWorst = (compareTimeSeries(bng2Cdat, rescaled)[0]?.maxRelErr ?? 0);
        if (rescaledWorst < rawWorst) {
          log(`      [Compartment] Rescaling CDAT unit-space: worst error ${rawWorst.toExponential(3)} -> ${rescaledWorst.toExponential(3)}`);
          webCdat = rescaled;   // ‚Üê this line was missing
        }
      }
    }

    cdatDiffs = compareTimeSeries(bng2Cdat, webCdat);
    log(`      CDAT Diff: ${cdatDiffs.length} trajectories compared`);
  } else if (cdatFilesCompared && !deterministicLike) {
    log(`    [Parity] Skipping CDAT trajectory comparison for ${modelName} (method=${simulationMethod})`);
    cdatFilesCompared = false;
  }

  if (gdatFilesCompared && deterministicLike) {
    log(`    [Parity] Comparing observable trajectories (GDAT)...`);
    const bng2GdatRaw = readTextFileWithRetry(preparedFiles.bng2Gdat!);
    const webGdatRaw = readTextFileWithRetry(preparedFiles.webGdat!);
    log(`      Read BNG2 GDAT: ${preparedFiles.bng2Gdat} (${bng2GdatRaw.length} bytes)`);
    log(`      Read Web GDAT:  ${preparedFiles.webGdat} (${webGdatRaw.length} bytes)`);
    
    const bng2Gdat = parseDat(bng2GdatRaw);
    const webSeries = preparedFiles.webGdat!.toLowerCase().endsWith('.csv') ? parseCsv(webGdatRaw) : parseDat(webGdatRaw);
    log(`      BNG2 Gdat: ${bng2Gdat.length} timepoints, Web series: ${webSeries.length} timepoints`);
    
    gdatDiffs = compareTimeSeries(bng2Gdat, webSeries);
    log(`      GDAT Diff: ${gdatDiffs.length} trajectories compared`);
  } else if (gdatFilesCompared && !deterministicLike) {
    log(`    [Parity] Skipping GDAT trajectory comparison for ${modelName} (method=${simulationMethod})`);
    gdatFilesCompared = false;
  }

  const partial: Omit<LayeredReport, 'rootCause' | 'summary' | 'firstDivergingLayer'> = {
    model: modelName,
    simulationMethod,
    parameterDiffs,
    speciesDiffs,
    reactionDiffs,
    groupDiffs,
    cdatDiffs,
    gdatDiffs,
    netFilesCompared,
    cdatFilesCompared,
    gdatFilesCompared,
  };

  const { rootCause, firstDivergingLayer } = classifyRootCause(partial, { simMethod: simulationMethod });
  const report: LayeredReport = {
    ...partial,
    rootCause,
    firstDivergingLayer,
    summary: '',
  };
  report.summary = buildSummary(report);
  return report;
}

function parseCli(argv: string[]): CliOptions {
  const models: string[] = [];
  let all = false;
  let outPath = path.join('artifacts', 'parity_layer_report.json');
  let generateWebNet = true;
  let generateWebCdat = true;
  let verbose = true;
  let limit: number | undefined;
  let timeoutMs: number | undefined;
  const explicit: Partial<ModelFiles> = {};

  for (let i = 0; i < argv.length; i++) {
    const a = argv[i];
    if (a === '--all') {
      all = true;
      continue;
    }
    if (a === '--out') {
      outPath = argv[++i] ?? outPath;
      continue;
    }
    if (a === '--limit') {
      limit = parseInt(argv[++i] ?? '0', 10);
      continue;
    }
    if (a === '--timeoutMs') {
      timeoutMs = parseInt(argv[++i] ?? '0', 10);
      continue;
    }
    if (a === '--no-generate-web-net') {
      generateWebNet = false;
      continue;
    }
    if (a === '--no-generate-web-cdat') {
      generateWebCdat = false;
      continue;
    }
    if (a === '--verbose') {
      verbose = true;
      continue;
    }
    if (a === '--quiet') {
      verbose = false;
      continue;
    }
    if (a === '--bng2-net') { explicit.bng2Net = argv[++i]; continue; }
    if (a === '--web-net') { explicit.webNet = argv[++i]; continue; }
    if (a === '--bng2-cdat') { explicit.bng2Cdat = argv[++i]; continue; }
    if (a === '--web-cdat') { explicit.webCdat = argv[++i]; continue; }
    if (a === '--bng2-gdat') { explicit.bng2Gdat = argv[++i]; continue; }
    if (a === '--web-gdat') { explicit.webGdat = argv[++i]; continue; }
    if (!a.startsWith('-')) models.push(a);
  }

  const hasExplicit = Object.keys(explicit).length > 0;
  return {
    models: dedupeModels(models),
    all,
    outPath,
    generateWebNet,
    generateWebCdat,
    verbose,
    explicit: hasExplicit ? explicit : undefined,
    limit,
    timeoutMs,
  };
}

function discoverAllModelsFromConstants(): string[] {
  const constantsPath = path.join(ROOT, 'constants.ts');
  if (!fs.existsSync(constantsPath)) {
    throw new Error(`constants.ts not found at ${constantsPath}`);
  }

  const txt = fs.readFileSync(constantsPath, 'utf8');
  const setName = 'BNG2_COMPATIBLE_MODELS';
  const escapedSetName = escapeRegExp(setName);
  const setRegex = new RegExp(
    `${escapedSetName}\\s*=\\s*new\\s+Set(?:<[^>]+>)?\\s*\\(\\s*\\[([\\s\\S]*?)\\]\\s*\\)`,
    'm'
  );
  const setMatch = txt.match(setRegex);
  if (!setMatch) {
    throw new Error(`Could not parse ${setName} from constants.ts`);
  }

  const listBody = setMatch[1];
  const literals = [...listBody.matchAll(/["'`]([^"'`]+)["'`]/g)].map((m) => m[1]).filter(Boolean);
  const deduped = dedupeModels(literals);

  const excludedNoSimulate: string[] = [];
  const filtered = deduped.filter((model) => {
    const bnglPath = discoverBngl(model);
    const keep = hasSimulateCommand(bnglPath);
    if (!keep) excludedNoSimulate.push(model);
    return keep;
  });

  if (excludedNoSimulate.length > 0) {
    log(
      `[Discovery] Excluding ${excludedNoSimulate.length} models without simulate commands from --all (e.g., ${excludedNoSimulate.slice(0, 5).join(', ')})`,
      'warn'
    );
  }

  return filtered;
}

async function main() {
  const opts = parseCli(process.argv.slice(2));
  VERBOSE = opts.verbose;
  log(`[Main] Options: models=[${opts.models.join(', ')}], all=${opts.all}, out=${opts.outPath}, genNet=${opts.generateWebNet}, genCdat=${opts.generateWebCdat}, limit=${opts.limit}, timeoutMs=${opts.timeoutMs ?? 180000}`);
  if (opts.explicit) log(`       Explicit overrides: ${JSON.stringify(opts.explicit, null, 2)}`);

  let models = [...opts.models];
  if (opts.all) models = discoverAllModelsFromConstants();
  models = dedupeModels(models);
  if (opts.limit) models = models.slice(0, opts.limit);

  if (opts.explicit && models.length <= 1) {
    const name = models[0] ?? 'manual';
    const discovered = discoverFiles(name);
    const merged: ModelFiles = {
      ...discovered,
      ...opts.explicit,
      bnglPath: discovered.bnglPath,
    };
    const report = await analyzeModel(name, merged, opts);
    console.log(report.summary);
    const outAbs = path.isAbsolute(opts.outPath) ? opts.outPath : path.join(ROOT, opts.outPath);
    fs.mkdirSync(path.dirname(outAbs), { recursive: true });
    fs.writeFileSync(outAbs, JSON.stringify(report, null, 2), 'utf8');
    process.exit(report.rootCause === 'pass' || report.rootCause === 'threshold_only' ? 0 : 1);
  }

  if (models.length === 0) {
    console.log('Usage: npx tsx scripts/layered_parity_check.ts <model...> | --all [--out path]');
    console.log('       Optional: --no-generate-web-net --no-generate-web-cdat');
    process.exit(1);
  }

  const reports: LayeredReport[] = [];
  const TIMEOUT_MS = opts.timeoutMs ?? 180_000; // 3 minutes per model by default

  for (const model of models) {
    console.log(`Analyzing ${model}...`);
    try {
      const files = discoverFiles(model);
      
      // Use a timeout for the analysis
      const analysisPromise = analyzeModel(model, files, opts);
      const timeoutPromise = new Promise<never>((_, reject) => 
        setTimeout(() => reject(new Error(`Timeout analyzing ${model} after ${TIMEOUT_MS/1000}s`)), TIMEOUT_MS)
      );

      const rep = await Promise.race([analysisPromise, timeoutPromise]);
      reports.push(rep);
    } catch (e: any) {
      console.error(`[ERROR] Failed to analyze ${model}: ${e.message}`);
      // Add a dummy 'unknown' report so the sweep continues
      reports.push({
        model,
        simulationMethod: 'missing',
        rootCause: 'unknown',
        parameterDiffs: [],
        speciesDiffs: [],
        reactionDiffs: [],
        groupDiffs: [],
        cdatDiffs: [],
        gdatDiffs: [],
        netFilesCompared: false,
        cdatFilesCompared: false,
        gdatFilesCompared: false,
        firstDivergingLayer: 'none',
        summary: `Error or Timeout: ${e.message}`
      });
    }
  }

  const counts: Record<RootCause, number> = {
    pass: 0,
    threshold_only: 0,
    trajectory_accuracy_mismatch: 0,
    parameter_mismatch: 0,
    species_mismatch: 0,
    reaction_count_mismatch: 0,
    rate_constant_mismatch: 0,
    group_mismatch: 0,
    solver_or_steadystate: 0,
    unknown: 0,
  };

  console.log('\n' + '='.repeat(120));
  console.log('LAYERED PARITY REPORT');
  console.log('='.repeat(120));

  for (const r of reports) {
    counts[r.rootCause]++;
    const emoji = r.rootCause === 'pass' ? '' : r.rootCause === 'threshold_only' ? '' : '';
    const worst = [...r.gdatDiffs, ...r.cdatDiffs].sort((a, b) => b.maxRelErr - a.maxRelErr)[0];
    const maxErr = worst ? worst.maxRelErr.toExponential(2) : 'n/a';
    console.log(`  ${emoji} ${r.model.padEnd(40)} root_cause=${r.rootCause.padEnd(28)} rxn_diffs=${String(r.reactionDiffs.length).padStart(3)}  worst_traj_err=${maxErr}`);
  }

  console.log('\n' + '-'.repeat(80));
  console.log('Classification summary:');
  for (const [k, v] of Object.entries(counts)) {
    if (v > 0) console.log(`  ${k}: ${v}`);
  }

  const passing = counts.pass + counts.threshold_only;
  const deterministicReports = reports.filter((r) => r.simulationMethod === 'ode' || r.simulationMethod === 'unspecified');
  const deterministicPassing = deterministicReports.filter((r) => r.rootCause === 'pass' || r.rootCause === 'threshold_only').length;
  console.log(`\nTotal: ${reports.length} models, ${passing} passing, ${reports.length - passing} failing`);
  console.log(`Deterministic-only: ${deterministicReports.length} models, ${deterministicPassing} passing, ${deterministicReports.length - deterministicPassing} failing`);

  const outAbs = path.isAbsolute(opts.outPath) ? opts.outPath : path.join(ROOT, opts.outPath);
  fs.mkdirSync(path.dirname(outAbs), { recursive: true });
  fs.writeFileSync(outAbs, JSON.stringify(reports, null, 2), 'utf8');
  console.log(`\nDetailed report written to ${outAbs}`);

  process.exit(reports.length - passing > 0 ? 1 : 0);
}

main().catch((e) => {
  console.error(e);
  process.exit(1);
});
